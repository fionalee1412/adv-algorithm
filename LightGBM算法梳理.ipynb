{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "# LightGBM算法梳理\n## 1.LightGBM\n1.1 GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT 在工业界应用广泛，通常被用于点击率预测，搜索排序等任务。GBDT 也是各种数据挖掘竞赛的致命武器，据统计 Kaggle 上的比赛有一半以上的冠军方案都是基于 GBDT。\n\n## 2.LightGBM的起源\n传统的boosting算法（如GBDT和XGBoost）已经有相当好的效率，但是在如今的大样本和高维度的环境下，传统的boosting似乎在效率和可扩展性上不能满足现在的需求了，主要的原因就是传统的boosting算法需要对每一个特征都要扫描所有的样本点来选择最好的切分点，这是非常的耗时。为了解决这种在大样本高纬度数据的环境下耗时的问题，出现了Lightgbm 。\nLightgbm使用了如下两种解决办法：一是GOSS（Gradient-based One-Side Sampling, 基于梯度的单边采样），不是使用所用的样本点来计算梯度，而是对样本进行采样来计算梯度；二是EFB（Exclusive Feature Bundling， 互斥特征捆绑） ，这里不是使用所有的特征来进行扫描获得最佳的切分点，而是将某些特征进行捆绑在一起来降低特征的维度，是寻找最佳切分点的消耗减少。这样大大的降低的处理样本的时间复杂度，但在精度上，通过大量的实验证明，在某些数据集上使用Lightgbm并不损失精度，甚至有时还会提升精度。\n### 2.1 XGBoost的缺点\n1） 在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。-- 预排序方法（pre-sorted）：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。\n\n2）时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。\n### 2.2 起源\n传统的boosting算法（如GBDT和XGBoost）已经有相当好的效率，但是在如今的大样本和高维度的环境下，传统的boosting似乎在效率和可扩展性上不能满足现在的需求了，主要的原因就是传统的boosting算法需要对每一个特征都要扫描所有的样本点来选择最好的切分点，这是非常的耗时。为了解决这种在大样本高纬度数据的环境下耗时的问题，出现了Lightgbm 。\nLightgbm使用了如下两种解决办法：一是GOSS（Gradient-based One-Side Sampling, 基于梯度的单边采样），不是使用所用的样本点来计算梯度，而是对样本进行采样来计算梯度；二是EFB（Exclusive Feature Bundling， 互斥特征捆绑） ，这里不是使用所有的特征来进行扫描获得最佳的切分点，而是将某些特征进行捆绑在一起来降低特征的维度，是寻找最佳切分点的消耗减少。这样大大的降低的处理样本的时间复杂度，但在精度上，通过大量的实验证明，在某些数据集上使用Lightgbm并不损失精度，甚至有时还会提升精度。\n## 3.Histogram VS pre-sorted\n### Pre-sorted\nPre-sorted 算法需要的内存约是训练数据的两倍(2 * #data * #features* 4Bytes)，它需要用32位浮点(4Bytes)来保存 feature value，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位(4Bytes)的存储空间。因此是(2 * #data * #features* 4Bytes)。而对于 histogram 算法，则只需要(#data * #features * 1Bytes)的内存消耗，仅为 pre-sorted算法的1/8。因为 histogram 算法仅需要存储 feature bin value (离散化后的数值)，不需要原始的 feature value，也不用排序，而 bin value 用 1Bytes(256 bins) 的大小一般也就足够了。\n\n计算上的优势则是大幅减少了计算分割点增益的次数。对于每一个特征，pre-sorted 需要对每一个不同特征值都计算一次分割增益，代价是O(#feature*#distinct_values_of_the_feature)；而 histogram 只需要计算#bins次，代价是(#feature*#bins)。\n\n还有一个很重要的点是cache-miss。事实上，cache-miss对速度的影响是特别大的。预排序中有2个操作频繁的地方会造成cache miss，一是对梯度的访问，在计算gain的时候需要利用梯度，不同特征访问梯度的顺序都是不一样的，且是随机的，因此这部分会造成严重的cache-miss。二是对于索引表的访问，预排序使用了一个行号到叶子节点号的索引表（row_idx_to_tree_node_idx ），来防止数据切分时对所有的数据进行切分，即只对该叶子节点上的样本切分。在与level-wise进行结合的时候， 每一个叶子节点都要切分数据，这也是随机的访问。这样会带来严重的系统性能下降。而直方图算法则是天然的cache friendly。在直方图算法的第3个for循环的时候，就已经统计好了每个bin的梯度，因此，在计算gain的时候，只需要对bin进行访问，造成的cache-miss问题会小很多。\n\n最后，在数据并行的时候，用 histgoram 可以大幅降低通信代价。用 pre-sorted 算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst 在并行的时候也使用 histogram 进行通信。\n（数据并行的优化是Lightgbm的令一个亮点，这里不是特别理解，需要再深入研究）\n\n### Histogram[直方算法]\n 直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。\n ![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/lgb-histogram-bin1.png)\n \n 使用直方图算法有很多优点。首先，最明显就是内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用 8 位整型存储就足够了，内存消耗可以降低为原来的1/8。\n ![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/lgb-histogram-bin2.png)\n \n 然后在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data*#feature)优化到O(k*#features)。 　\n\n​ 当然，Histogram 算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。\n\u003e原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。 \n#### 直方图加速\n​ LightGBM 另一个优化是 Histogram（直方图）做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。\n ![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/lgb-histogram-accelerate.png)\n\n## 4.leaf-wise VS level-wise\n​ 在 Histogram 算法之上，LightGBM 进行进一步的优化。首先它抛弃了大多数 GBDT 工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。Level-wise 过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上 Level-wise 是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。\n ![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/lgb-level-wise-tree.png)\n\n\n Leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。\n ![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/lgb-leaf-wise-tree.png)\n## 5.特征并行和数据并行\nLightGBM 还具有支持高效并行的优点。LightGBM 原生支持并行学习，目前支持特征并行和数据并行的两种。\n\n- 特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。\n- 数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。\n​ LightGBM 针对这两种并行方法都做了优化：\n\n在特征并行算法中，通过在本地保存全部数据避免对数据切分结果的通信；\n在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。\n ![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/lgb-feature-parallel-opsimiter1.jpg)\n ![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/lgb-feature-parallel-opsimiter2.jpg)\n ![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/lgb-feature-parallel-opsimiter3.jpg)\n#### 注意\n- 当生长相同的叶子时，Leaf-wise 比 level-wise 减少更多的损失。\n- 高速，高效处理大数据，运行时需要更低的内存，支持 GPU\n- 不要在少量数据上使用，会过拟合，建议 10,000+ 行记录时使用。\n\n## 6.顺序访问梯度\n预排序算法中有两个频繁的操作会导致cache-miss，也就是缓存消失（对速度的影响很大，特别是数据量很大的时候，顺序访问比随机访问的速度快4倍以上 ）。\n\n对梯度的访问：在计算增益的时候需要利用梯度，对于不同的特征，访问梯度的顺序是不一样的，并且是随机的\n对于索引表的访问：预排序算法使用了行号和叶子节点号的索引表，防止数据切分的时候对所有的特征进行切分。同访问梯度一样，所有的特征都要通过访问这个索引表来索引。\n这两个操作都是随机的访问，会给系统性能带来非常大的下降。\n\nLightGBM使用的直方图算法能很好的解决这类问题。首先。对梯度的访问，因为不用对特征进行排序，同时，所有的特征都用同样的方式来访问，所以只需要对梯度访问的顺序进行重新排序，所有的特征都能连续的访问梯度。并且直方图算法不需要把数据id到叶子节点号上（不需要这个索引表，没有这个缓存消失问题）\n## 7.支持类别特征\n实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的0/1 特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM 优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1 展开。并在决策树算法上增加了类别特征的决策规则。在 Expo 数据集上的实验，相比0/1 展开的方法，训练速度可以加速 8 倍，并且精度一致。据我们所知，LightGBM 是第一个直接支持类别特征的 GBDT 工具。\n LightGBM 的单机版本还有很多其他细节上的优化，比如 cache 访问优化，多线程优化，稀疏特征优化等等。优化汇总如下：\n \n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n  \u003cth align\u003d\"center\"\u003e类目\u003c/th\u003e\n  \u003cth align\u003d\"center\"\u003e预排序算法预排序算法\u003c/th\u003e\n  \u003cth align\u003d\"center\"\u003elightGBM\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n  \u003ctd align\u003d\"center\"\u003e内存占用\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e\u003ccode\u003e2*#feature*#data*4Bytes\u003c/code\u003e\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e\u003ccode\u003e*#feature*#data*1Bytes\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd align\u003d\"center\"\u003e统计量累积\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e\u003ccode\u003eO(*#feature*#data)\u003c/code\u003e\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e\u003ccode\u003eO(*#feature*#data)\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd align\u003d\"center\"\u003e分割增益计算\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e\u003ccode\u003eO(*#feature*#data)\u003c/code\u003e\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e\u003ccode\u003eO(*#feature*#k)\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd align\u003d\"center\"\u003e直方图做差\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003eN/A\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e加速一倍\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd align\u003d\"center\"\u003e直接支持类别特征\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003eN/A\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e在Expo数据上加速8倍\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd align\u003d\"center\"\u003eCache优化\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003eN/A\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e在Higgs数据上加速40%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd align\u003d\"center\"\u003e带深度限制的Leaf-wise的决策树算法\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003eN/A\u003c/td\u003e\n  \u003ctd align\u003d\"center\"\u003e精度更\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n## 8.应用场景\n## 9.sklearn参数\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import lightgbm as lgb\nparams \u003d {  \n    \u0027boosting_type\u0027: \u0027gbdt\u0027,  \n    \u0027objective\u0027: \u0027binary\u0027,  \n    \u0027metric\u0027: {\u0027binary_logloss\u0027, \u0027auc\u0027},  #二进制对数损失\n    \u0027num_leaves\u0027: 5,  \n    \u0027max_depth\u0027: 6,  \n    \u0027min_data_in_leaf\u0027: 450,  \n    \u0027learning_rate\u0027: 0.1,  \n    \u0027feature_fraction\u0027: 0.9,  \n    \u0027bagging_fraction\u0027: 0.95,  \n    \u0027bagging_freq\u0027: 5,  \n    \u0027lambda_l1\u0027: 1,    \n    \u0027lambda_l2\u0027: 0.001,  # 越小l2正则程度越高  \n    \u0027min_gain_to_split\u0027: 0.2,  \n    \u0027verbose\u0027: 5,  \n    \u0027is_unbalance\u0027: True  \n}\nlgb.train(params, None, num_boost_round\u003d100,\n          valid_sets\u003dNone, valid_names\u003dNone,\n          fobj\u003dNone, feval\u003dNone, init_model\u003dNone,\n          feature_name\u003d\u0027auto\u0027, categorical_feature\u003d\u0027auto\u0027,\n          early_stopping_rounds\u003dNone, evals_result\u003dNone,\n          verbose_eval\u003dTrue, learning_rates\u003dNone,\n          keep_training_booster\u003dFalse, callbacks\u003dNone):",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 3.1 控制参数\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n  \u003cth\u003eControl Parameters\u003c/th\u003e\n  \u003cth\u003e含义\u003c/th\u003e\n  \u003cth\u003e用法\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003emax_depth\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e树的最大深度\u003c/td\u003e\n  \u003ctd\u003e当模型过拟合时,可以考虑首先降低 \u003ccode\u003emax_depth\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003emin_data_in_leaf\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e叶子可能具有的最小记录数\u003c/td\u003e\n  \u003ctd\u003e默认20，过拟合时用\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003efeature_fraction\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e例如 为0.8时，意味着在每次迭代中随机选择80％的参数来建树\u003c/td\u003e\n  \u003ctd\u003eboosting 为 random forest 时用\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003ebagging_fraction\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e每次迭代时用的数据比例\u003c/td\u003e\n  \u003ctd\u003e用于加快训练速度和减小过拟合\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003eearly_stopping_round\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e如果一次验证数据的一个度量在最近的\u003ccode\u003eearly_stopping_round\u003c/code\u003e 回合中没有提高，模型将停止训练\u003c/td\u003e\n  \u003ctd\u003e加速分析，减少过多迭代\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003elambda\u003c/td\u003e\n  \u003ctd\u003e指定正则化\u003c/td\u003e\n  \u003ctd\u003e0～1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003emin_gain_to_split\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e描述分裂的最小 gain\u003c/td\u003e\n  \u003ctd\u003e控制树的有用的分裂\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003emax_cat_group\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e在 group 边界上找到分割点\u003c/td\u003e\n  \u003ctd\u003e当类别数量很多时，找分割点很容易过拟合时\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n### 3.2 核心参数\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n  \u003cth\u003eCoreParameters\u003c/th\u003e\n  \u003cth\u003e含义\u003c/th\u003e\n  \u003cth\u003e用法\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003eTask\u003c/td\u003e\n  \u003ctd\u003e数据的用途\u003c/td\u003e\n  \u003ctd\u003e选择 train 或者 predict\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003eapplication\u003c/td\u003e\n  \u003ctd\u003e模型的用途\u003c/td\u003e\n  \u003ctd\u003e选择 regression: 回归时，binary: 二分类时，multiclass: 多分类时\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003eboosting\u003c/td\u003e\n  \u003ctd\u003e要用的算法\u003c/td\u003e\n  \u003ctd\u003egbdt， rf: random forest， dart: Dropouts meet Multiple Additive Regression Trees， goss: \u003ccode\u003eGradient-based One-Side Sampling\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003enum_boost_round\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e迭代次数\u003c/td\u003e\n  \u003ctd\u003e通常 100+\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003elearning_rate\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e如果一次验证数据的一个度量在最近的 \u003ccode\u003eearly_stopping_round\u003c/code\u003e 回合中没有提高，模型将停止训练\u003c/td\u003e\n  \u003ctd\u003e常用 0.1, 0.001, 0.003…\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003enum_leaves\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003e默认 31\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003edevice\u003c/td\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003ecpu 或者 gpu\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003emetric\u003c/td\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003ccode\u003emae: mean absolute error ， mse: mean squared error ， binary_logloss: loss for binary classification ， multi_logloss: loss for multi classification\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n### 3.3 IO参数\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n  \u003cth\u003eIO parameter\u003c/th\u003e\n  \u003cth\u003e含义\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003emax_bin\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e表示 feature 将存入的 bin 的最大数量\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003ecategorical_feature\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e如果 \u003ccode\u003ecategorical_features \u003d 0,1,2\u003c/code\u003e， 则列 0，1，2是 categorical 变量\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003eignore_column\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e与 \u003ccode\u003ecategorical_features\u003c/code\u003e 类似，只不过不是将特定的列视为categorical，而是完全忽略\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003esave_binary\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e这个参数为 true 时，则数据集被保存为二进制文件，下次读数据时速度会变快\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n### 3.4 调参\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n  \u003cth\u003eIO parameter\u003c/th\u003e\n  \u003cth\u003e含义\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003enum_leaves\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e取值应 \u003ccode\u003e\u0026lt;\u003d 2 ^（max_depth）\u003c/code\u003e， 超过此值会导致过拟合\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003emin_data_in_leaf\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e将它设置为较大的值可以避免生长太深的树，但可能会导致 underfitting，在大型数据集时就设置为数百或数千\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003ccode\u003emax_depth\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e这个也是可以限制树的深度\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n下表对应了 Faster Speed ，better accuracy ，over-fitting 三种目的时，可以调的参数\n\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n  \u003cth\u003eFaster Speed\u003c/th\u003e\n  \u003cth\u003ebetter accuracy\u003c/th\u003e\n  \u003cth\u003eover-fitting\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n  \u003ctd\u003e将 \u003ccode\u003emax_bin\u003c/code\u003e 设置小一些\u003c/td\u003e\n  \u003ctd\u003e用较大的 \u003ccode\u003emax_bin\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003ccode\u003emax_bin\u003c/code\u003e 小一些\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003ccode\u003enum_leaves\u003c/code\u003e 大一些\u003c/td\u003e\n  \u003ctd\u003e\u003ccode\u003enum_leaves\u003c/code\u003e 小一些\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e用 \u003ccode\u003efeature_fraction\u003c/code\u003e 来做 \u003ccode\u003esub-sampling\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003e用 \u003ccode\u003efeature_fraction\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e用 \u003ccode\u003ebagging_fraction 和 bagging_freq\u003c/code\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003e设定 \u003ccode\u003ebagging_fraction 和 bagging_freq\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003etraining data 多一些\u003c/td\u003e\n  \u003ctd\u003etraining data 多一些\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e用 \u003ccode\u003esave_binary\u003c/code\u003e 来加速数据加载\u003c/td\u003e\n  \u003ctd\u003e直接用 categorical feature\u003c/td\u003e\n  \u003ctd\u003e用 \u003ccode\u003egmin_data_in_leaf 和 min_sum_hessian_in_leaf\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e用 parallel learning\u003c/td\u003e\n  \u003ctd\u003e用 dart\u003c/td\u003e\n  \u003ctd\u003e用 \u003ccode\u003elambda_l1, lambda_l2 ，min_gain_to_split\u003c/code\u003e 做正则化\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003ccode\u003enum_iterations\u003c/code\u003e 大一些，\u003ccode\u003elearning_rate\u003c/code\u003e 小一些\u003c/td\u003e\n  \u003ctd\u003e用 \u003ccode\u003emax_depth\u003c/code\u003e 控制树的深度\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n## 10.CatBoost(了解)\nhttps://zhuanlan.zhihu.com/p/37916954",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}