{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# GBDT算法梳理\n![GBDT_TRER](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/gbdt_tree.jpg)\n\nGBDT:gradient boosting decision tree 梯度增强决策树\n\nGBDT是一种采用加法模型（即基函数的线性组合）与前向分步算法并以决策树作为基函数的提升方法。通俗来说就是，该算法由多棵决策树组成，所有树的结论加起来形成最终答案。\n## 1.前向分布算法\n   - 1.1 加载模型\n   \n   $f(x)\u003d\\displaystyle\\sum^M_{m\u003d1}\\beta_mb(x;\\gamma_m )$ (及学习器的一种线性模型)\n   \n   其中，$b(x;\\gamma_m )$ 为基函数，$\\gamma_m$作为基函数的参数，$\\beta_m$作为基函数的系数。\n   \n   在给定训练数据的损失函数$L(y,f(x))$的条件下，学习加法模型成为损失函数极小化问题：\n   \n   $min_{\\beta_m,\\gamma_m}\\displaystyle\\sum^N_{i\u003d1}L(y_i,\\displaystyle\\sum^M_{m\u003d1}\\beta_mb(x_i;\\gamma_m))$(同时求解那么多参数)\n   \n   前向分步算法求解这一优化问题的思路：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步去逼近上述的目标函数式，就可简化优化的复杂度，每一步只需优化如下损失函数：\n   \n   $min_{\\beta_m,\\gamma_m}\\displaystyle\\sum^N_{i\u003d1}L(y_i,\\beta_mb(x;\\gamma))$(每步学习一个基函数和系数)\n   \n### 前向分步算法流程：\n\n--------------------------------------------------------------------------------------------\n\n输入：训练数据集T\u003d$({ (x_1,y_1),(x_2,y_2),...(x_N,y_N) })$；损失函数$L(y,f(x))$；基函数集${ b(x,\\gamma) }$；\n\n输出：加法模型f(x)\n\n(1) 初始化 $f_0(x)\u003d0$\n\n(2) 对m\u003d1,2,...,M\n\n(a) 极小化损失函数\n\n$(\\beta,\\gamma)\u003dargmin_{\\beta,\\gamma}\\displaystyle\\sum^N_{i\u003d1}L(y_i,f_{m-1}（x_i）+\\beta b(x_i;\\gamma))$\n\n得到参数 $\\beta_m,\\gamma_m$\n\n(b)更新\n\n$f_m(x)\u003df_{m-1}+\\beta_m b(x;\\gamma_m)$\n\n(3)得到加法模型\n\n$f(x)\u003df_M(x)\u003d\\displaystyle\\sum^M_{m\u003d1}\\beta_m b(x;\\beta_m)$\n\n------------------------------------------------------------------------------------------\n\n可见，前向分步算法将同时求解从m\u003d1到M所有参数 $\\beta_m,\\gamma_m$ 的优化问题简化成逐步求解各个 $\\beta_m,\\gamma_m$的优化问题了\n## 2.负梯度拟合\n### 2.1.提升树算法\n\n了解完前向分步算法，再来看看什么是提升树算法。\n\n提升方法实际采用加法模型与前向分步算法，以决策树作为基函数的提升方法称为提升树。注意，这里的决策树为CART回归树，不是分类树。当问题是分类问题时，采用的决策树模型为分类回归树。为什么要采用决策树作为基函数呢？它有以下优缺点：\n\n优点\n\n可解释性强\n可处理混合类型特征\n具有伸缩不变性（不用归一化特征）\n有特征组合的作用\n可自然处理缺失值\n对异常点鲁棒\n有特征选择作用\n可扩展性强，容易并行\n缺点\n\n缺乏平滑性（回归预测时输出值只能是输出有限的若干种数值）\n不适合处理高维稀疏数据\n由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂也是如此。\n\n提升树模型可表示为：\n\n$f_M(x)\u003d\\displaystyle\\sum^M_{m\u003d1}T(x;\\theta_m)$\n\n其中， $T(x;\\theta_m)$ 表示决策树;$\\theta$ 为决策树的参数;M为树的个数;M为树的个数。\n\n针对不同的问题，提升树算法的形式有所不同，其主要区别在于使用的损失函数不同。而损失函数的不同，决策树要拟合的值也会不同。就一般而言，对于回归问题的提升树算法来说，若损失函数是平方损失函数，每一步只需简单拟合当前模型的残差。\n\n下面看一下在回归问题下，损失函数为平方损失函数的算法流程：\n\n--------------------------------------------------------------------------------------------\n\n输入：训练数据集T\u003d$({ (x_1,y_1),(x_2,y_2),...(x_N,y_N) }),x_i \\in \\chi \\subseteq R^n,y_i \\in \\gamma \\subseteq R$ ；\n\n输出：提升树 $f_M(x)$\n\n(1)初始化 $f_0(x)\u003d0$\n\n(2)对m\u003d1,2,...,M\n\n(a)计算残差\n\n$r_{mi}\u003dy_i-f_{m-1}(x_i),i\u003d1,2,...N$\n\n(b)拟合残差 $r_{mi}$ 学习一个回归树，得到 $T(x;\\theta_m)$\n\n(c)更新$f_m(x)\u003df_{m-1}(x)+T(x;\\theta_m)$\n\n(3)得到回归问题提升树\n\n$f_M(x)\u003d\\displaystyle\\sum^M_{m\u003d1}T(x;\\theta_m)$\n\n--------------------------------------------------------------------------------------------\n\n2.梯度提升\n\n提升树用加法模型与前向分布算法实现学习的优化过程。当损失函数为平方损失和指数损失函数时，每一步优化是很简单的。但对于一般损失函数而言，往往每一步都不那么容易。对于这问题，Freidman提出了梯度提升算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值：\n\n$-[\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}]_{f(x)}\u003df_{m-1}(x)$\n\n作为回归问题在当前模型的残差的近似值，拟合一个回归树。\n\n为什么要拟合负梯度呢？这就涉及到泰勒公式和梯度下降法了。\n\n泰勒公式的形式是这样的：\n\n定义：泰勒公式是一个用函数在某点的信息描述其附近取值的公式。\n\n基本形式： $f(x)\u003d\\displaystyle\\sum^\\infty_{n\u003d1}\\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n$\n\n一阶泰勒展开： $f(x)\\approx f(x_0)+f\u0027(x_0)(x-x_0)$\n\n梯度下降法\n\n在机器学习任务中，需要最小化损失函数 $L(\\theta)$ ，其中$\\theta$ 是要求解的模型参数。梯度下降法常用来求解这种无约束最优化问题，它是一种迭代方法：选择初值 $\\theta^0$ ，不断迭代更新 $\\theta$ 的值，进行损失函数极小化。\n\n迭代公式： $\\theta^t\u003d\\theta^{t-1}+\\Delta\\theta$\n\n将 $L(\\theta^t)$ 在 $\\theta^{t-1}$ 处进行一阶泰勒展开： $L(\\theta^t)\u003dL(\\theta^{t-1}+\\Delta\\theta))\\approx L(\\theta^{t-1})+L\u0027(\\theta^{t-1})\\Delta\\theta$\n\n要使得 $L(\\theta^t)\u003cL(\\theta^{t-1})$ ，可取： $\\Delta\\theta\u003d-\\alpha L\u0027(\\theta^{t-1})$ ，\n\n则： $\\theta\u003d\\theta^{t-1}-\\alpha L\u0027(\\theta^{t-1})$ ，这里 $\\alpha$ 是步长，一般直接赋值一个小的数。\n\n相对的，在函数空间里，有 $f^t(x)\u003df^{t-1}(x)+f_t(x)$\n\n此处把 $L(f^t(x))$ 看成提升树算法的第t步损失函数的值， $L(f^{t-1}(x))$ 为第t-1步损失函数值，要使 $L(f^t(x))\u003cL(f^{t-1}(x))$ ，\n\n则需要 $f_t(x)\u003d-\\alpha g_t(x)$ ，此处 $-g_t(x)$ 为当前模型的负梯度值，即第t步的回归树需要拟合的值。\n\n至于GBDT的具体算法流程，在后续回归与分类问题会分开说明。\n\n## 3.损失函数\n\n在GBDT算法中，损失函数的选择十分重要。针对不同的问题，损失函数有不同的选择。\n\n1.对于分类算法，其损失函数一般由对数损失函数和指数损失函数两种。\n\n(1)指数损失函数表达式：\n\n$L(y,f(x))\u003de^{(-yf(x))}$\n\n(2)对数损失函数可分为二分类和多分类两种。\n\n2.对于回归算法，常用损失函数有如下4种。\n\n(1)平方损失函数：\n\n$L(y,f(x))\u003d(y-f(x))^2$\n\n(2)绝对损失函数：\n\n$L(y,f(x))\u003d|y-f(x)|$\n\n对应负梯度误差为：\n\n$L(y,f(x))\u003dsign(y_i-f(x_i))$\n\n(3)Huber损失，它是均方差和绝对损失的折中产物，对于远离中心的异常点，采用绝对损失误差，而对于靠近中心的点则采用平方损失。这个界限一般用分位数点度量。损失函数如下：\n\n$L(y,f(x))\u003d\\left\\{\\begin{matrix} \\frac{1}{2}(y-f(x))^2\u0026  |y-f(x)| \\leqslant\\delta\u0026  \\\\ \\delta(|y-f(x)|-\\frac{\\delta}{2})\u0026  |y-f(x)|\u003e\\delta\u0026 \\end{matrix}\\right.$\n\n对应的负梯度误差为：\n\n$r(y_i,f(x_i))\u003d\\left\\{\\begin{matrix}y_i-f(x_i)\u0026  |y_i-f(x)|\\leqslant\\delta\u0026  \\\\ \\delta sign(y_i-f(_i))\u0026  |y_i-f(x_i)|\u003e\\delta\u0026 \\end{matrix}\\right.$\n\n\n（4）分位数损失。它对应的是分位数回归的损失函数，表达式为：\n\n$L(y,f(x))\u003d\\displaystyle\\sum_{y \\geqslant f(x)} \\theta|y-f(x)|+\\displaystyle\\sum_{y \u003c f(x)} (1-\\theta)|y-f(x)|$\n\n其中 $\\theta$ 为分位数，需要我们在回归之前指定。对应的负梯度误差为：\n\n$r(y_i,f(x_i))\\left\\{\\begin{matrix}\\theta\u0026  y_i \\geqslant f(x_i)\u0026  \\\\ \\theta-1 \u0026  y_i \\geqslant f(x_i)\u0026 \\end{matrix}\\right.$\n\n对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。\n## 4.回归\n\n梯度提升算法（回归问题）流程：\n\n--------------------------------------------------------------------------------------------\n\n输入：训练数据集T\u003d$({ (x_1,y_1),(x_2,y_2),...(x_N,y_N) }),x_i \\in \\chi \\subseteq R^n,y_i \\in \\gamma \\subseteq R$损失函数L(y,f(x))；\n\n输出：回归树 $\\tilde f(x)$\n\n(1)初始化\n\n$f_0\u003dargmin_c \\displaystyle \\sum^N_{i\u003d1}L(y_i,c)$ 注：估计使损失函数极小化的常数值，它是只有一个根结点的树\n\n(2)对m\u003d1,2,...,M\n\n(a)对i\u003d1,2,...N，计算\n\n$r_{mi}\u003d-[\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}]_{f(x)}\u003df_{m-1}(x)$ 注： 计算损失函数在当前模型的值，作为残差的估计\n\n(b)对 $r_{mi}$ 拟合一个回归树，得到第m棵树的叶结点区域 $R_{mj}$ ,j\u003d1,2,...,J\n\n(c)对j\u003d1,2,...,J，计算\n\n$c_{mj}\u003dargmin_c \\displaystyle \\sum _{x_j \\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$ 注：在损失函数极小化条件下，估计出相应叶结点区域的值\n\n(d)更新\n\n$f_m(x)\u003df_{m-1}(x)+ \\displaystyle \\sum^J_{j\u003d1}c_{mj}I(x \\in R_{mj})$\n\n(3)得到回归树\n\n$\\tilde f(x)\u003dfM(x)\u003d\\displaystyle\\sum^M_{m\u003d1} \\displaystyle\\sum^J_{j\u003d1} c_{mj}I(x \\in R_{mj})$\n\n--------------------------------------------------------------------------------------------\n## 5.二分类，多分类\n\n这里看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合输出类别的误差。\n\n为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法用类似逻辑回归的对数似然函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。此处仅讨论用对数似然函数的GBDT分类。对于对数似然损失函数，我们有又有二元分类和的多元分类的区别。\n\n1.二分类GBDT算法\n\n对于二分类GBDT，如果用类似逻辑回归的对数似然损失函数，则损失函数为：\n\n$L(y,f(x))\u003dlog(1+exp(-yf(x)))$\n\n其中 $y \\in ${-1,1} 。此时的负梯度误差为：\n\n$r_{ti}\u003d-\\frac{\\partial L(y_i,f_{t-1})}{\\partial f_{t-1}(x_i)}\u003d\\frac{y_i}{1+exp(y_i f_{t-1}(x_i))},i\u003d1,2,3...,m$\n\n对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为\n\n$c_{tj}\u003dargmin_c \\displaystyle \\sum_{x_i \\in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i)+c)))$\n\n由于上式比较难优化，我们一般使用近似值代替\n\n$c_{tj}\u003d\\frac{\\displaystyle \\sum_{x_i \\in R_{tj}} r_{tj}}{\\displaystyle \\sum_{x_i \\in R_{tj}}|r_{tj}|(1-|r_{tj}|)}$\n\n除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，二分类GBDT与GBDT回归算法过程相同。\n\n2.多分类GBDT算法\n\n多分类GBDT比二分类GBDT复杂些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：\n\n$L(y,f(x))-\\displaystyle \\sum^K_{k\u003d1} y_k log(p_k(x))$\n\n其中如果样本输出类别为k，则 $y_k$ \u003d1.第k类的概率 $p_k(x)$ 的表达式为：\n\n$p_k(x)\u003d\\frac{exp(f_k(x))}{\\sum^K_{l\u003d1}}exp(f_l(x))$\n\n集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为：\n\n$t_{til}\u003d-[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}]_{f_k(x)\u003df_{l,t-1}(x)}\u003dy_{il}-p_{l,t-1}(x_i)$\n\n观察上式可以看出，其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。\n\n对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为：\n\n$c_{tjl}\u003dargmin_{c_{jl}} \\displaystyle\\sum^m_{i\u003d1} \\displaystyle\\sum^K_{k\u003d1} L(y_k,f_{t-1,l}(x)+\\displaystyle\\sum^J_{j\u003d1} c_{jl}I(x_i \\in R_{tj}))$\n\n由于上式比较难优化，我们一般使用近似值代替\n\n$c_{tjl}\u003d\\frac{K-1}{K}\\frac{\\sum_{x_i \\in R_{tjl}} r_{til}}{\\sum_{x_i \\in R_{tjl}}|r_{til}|(1-|r_{til}|)}$\n\n除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多分类GBDT与二分类GBDT以及GBDT回归算法过程相同。\n\n## 6.正则化\n\n对GBDT进行正则化来防止过拟合，主要有三种形式。\n1.给每棵数的输出结果乘上一个步长$\\alpha$（learning rate）。\n\n对于前面的弱学习器的迭代：\n\n$f_m(x)\u003df_{m-1}(x)+T(x;\\gamma_m)$\n\n加上正则化项，则有\n\n$f_m(x)\u003df_{m-1}(x)+\\alpha T(x;\\gamma_m)$\n\n此处，$\\alpha$的取值范围为(0,1]。对于同样的训练集学习效果，较小的$\\alpha$意味着需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起决定算法的拟合效果。\n\n2.第二种正则化的方式就是通过子采样比例(subsample)。取值范围为(0,1]。\n\nGBDT这里的做法是在每一轮建树时，样本是从原始训练集中采用无放回随机抽样的方式产生，与随机森立的有放回抽样产生采样集的方式不同。若取值为1，则采用全部样本进行训练，若取值小于1，则不选取全部样本进行训练。选择小于1的比例可以减少方差，防止过拟合，但可能会增加样本拟合的偏差。取值要适中，推荐[0.5,0.8]。\n\n3.第三种是对弱学习器即CART回归树进行正则化剪枝。（如控制树的最大深度、节点的最少样本数、最大叶子节点数、节点分支的最小样本数等\n\n## 7.优缺点\n\n7.1.GBDT优点\n\n可以灵活处理各种类型的数据，包括连续值和离散值。\n在相对较少的调参时间情况下，预测的准确率也比较高，相对SVM而言。\n在使用一些健壮的损失函数，对异常值得鲁棒性非常强。比如Huber损失函数和Quantile损失函数。\n\n7.2.GBDT缺点\n\n由于弱学习器之间存在较强依赖关系，难以并行训练。可以通过自采样的SGBT来达到部分并行。\n\n## 8.sklearn参数\n在scikit-learning中，GradientBoostingClassifier对应GBDT的分类算法，GradientBoostingRegressor对应GBDT的回归算法。\n\n具体算法参数情况如下：",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "from sklearn.ensemble import GradientBoostingRegressor\n\nGradientBoostingRegressor(loss\u003d\u0027ls\u0027, learning_rate\u003d0.1, n_estimators\u003d100, \n                subsample\u003d1.0, criterion\u003d\u0027friedman_mse\u0027, min_samples_split\u003d2,\n                min_samples_leaf\u003d1, min_weight_fraction_leaf\u003d0.0, max_depth\u003d3,\n                min_impurity_decrease\u003d0.0, min_impurity_split\u003dNone, init\u003dNone, \n                random_state\u003dNone, max_features\u003dNone, alpha\u003d0.9, verbose\u003d0, \n                max_leaf_nodes\u003dNone, warm_start\u003dFalse, presort\u003d\u0027auto\u0027, \n                validation_fraction\u003d0.1, n_iter_no_change\u003dNone, tol\u003d0.0001)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "参数说明：\n\n- n_estimators：弱学习器的最大迭代次数，也就是最大弱学习器的个数。\n- learning_rate：步长，即每个学习器的权重缩减系数a，属于GBDT正则化方化手段之一。\n- subsample：子采样，取值(0,1]。决定是否对原始数据集进行采样以及采样的比例，也是GBDT正则化手段之一。\n- init：我们初始化的时候的弱学习器。若不设置，则使用默认的。\n- loss：损失函数，可选{\u0027ls\u0027-平方损失函数，\u0027lad\u0027绝对损失函数-,\u0027huber\u0027-huber损失函数,\u0027quantile\u0027-分位数损失函数}，默认\u0027ls\u0027。\n- alpha：当我们在使用Huber损失\"Huber\"和分位数损失\"quantile\"时，需要指定相应的值。默认是0.9，若噪声点比较多，可适当降低这个分位数值。\n- criterion：决策树节搜索最优分割点的准则，默认是\"friedman_mse\"，可选\"mse\"-均方误差与\u0027mae\"-绝对误差。\n- max_features：划分时考虑的最大特征数，就是特征抽样的意思，默认考虑全部特征。\n- max_depth：树的最大深度。\n- min_samples_split：内部节点再划分所需最小样本数。\n- min_samples_leaf：叶子节点最少样本数。\n- max_leaf_nodes：最大叶子节点数。\n- min_impurity_split：节点划分最小不纯度。\n- presort：是否预先对数据进行排序以加快最优分割点搜索的速度。默认是预先排序，若是稀疏数据，则不会预先排序，另外，稀疏数据不能设置为True。\n- validationfraction：为提前停止而预留的验证数据比例。当n_iter_no_change设置时才能用。\n- n_iter_no_change：当验证分数没有提高时，用于决定是否使用早期停止来终止训练。\n\n## 9.应用场景\n\n问题：\n对数损失函数的推导过程\n\nhttps://zhuanlan.zhihu.com/p/35709139\n\nhttps://www.csuldw.com/2016/03/26/2016-03-26-loss-function/\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}